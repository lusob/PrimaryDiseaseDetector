{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrimaryDiseaseDetector.ipynb\n",
    "\n",
    "# Importing necessary libraries\n",
    "\n",
    "# Standard utilities\n",
    "import os\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuration\n",
    "RETRAIN_MODEL = False  # Set to True to train a new model, False to load an existing one\n",
    "model_file = \"model/PrimaryDiseaseDetectorModel.keras\"\n",
    "\n",
    "# Function to download files from Google Drive\n",
    "def download_from_google_drive(url, output_path):\n",
    "    file_id = url.split('/d/')[1].split('/')[0]\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n",
    "\n",
    "# Google Drive URLs\n",
    "tcga_dataset_log2_url = \"https://drive.google.com/file/d/1-6OA1Q0TqFeooVHmURcZ_F9YjRh9D2cK/view?usp=drive_link\"\n",
    "met500_dataset_log2_url = \"https://drive.google.com/file/d/1nBzGFuq-ExWw0KC0dtagJqAOFjji8bQc/view?usp=drive_link\"\n",
    "phenotype_tcga_url = \"https://drive.google.com/file/d/1wNXgjZMQUDqNosG_q8qZNIIq0za-ghF0/view?usp=drive_link\"\n",
    "phenotype_met500_url = \"https://drive.google.com/file/d/1-7yVlLwIo2aD_eojIysUllnRXb3j-b7e/view?usp=drive_link\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Load or download and process data depending on RETRAIN_MODEL\n",
    "if RETRAIN_MODEL:\n",
    "    print(\"Downloading TCGA data...\")\n",
    "    download_from_google_drive(tcga_dataset_log2_url, \"data/tcga_gene_expression_log2_common_genes.csv\")\n",
    "\n",
    "    print(\"Downloading MET500 data...\")\n",
    "    download_from_google_drive(met500_dataset_log2_url, \"data/met500_gene_expression_common_genes.csv\")\n",
    "\n",
    "    print(\"Downloading TCGA phenotypes...\")\n",
    "    download_from_google_drive(phenotype_tcga_url, \"data/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\")\n",
    "\n",
    "    print(\"Downloading MET500 phenotypes...\")\n",
    "    download_from_google_drive(phenotype_met500_url, \"data/MET500_metadata.txt\")\n",
    "\n",
    "    # Load datasets\n",
    "    tcga_df_log2 = pd.read_csv(\"data/tcga_gene_expression_log2_common_genes.csv\", index_col=0)\n",
    "    met500_df = pd.read_csv(\"data/met500_gene_expression_common_genes.csv\", index_col=0)\n",
    "    phenotype_tcga = pd.read_csv(\"data/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\", sep=\"\\t\").set_index(\"sample\")\n",
    "    phenotype_met500 = pd.read_csv(\"data/MET500_metadata.txt\", sep=\"\\t\").set_index(\"Sample_id\")\n",
    "\n",
    "    # Verify dataset dimensions\n",
    "    print(f\"TCGA dimensions: {tcga_df_log2.shape}\")\n",
    "    print(f\"MET500 dimensions: {met500_df.shape}\")\n",
    "    print(f\"TCGA phenotypes dimensions: {phenotype_tcga.shape}\")\n",
    "    print(f\"MET500 phenotypes dimensions: {phenotype_met500.shape}\")\n",
    "\n",
    "    # Normalization and data preprocessing\n",
    "    scaler = MinMaxScaler()\n",
    "    tcga_scaled = scaler.fit_transform(tcga_df_log2.T)\n",
    "    met500_scaled = scaler.transform(met500_df.T)\n",
    "\n",
    "    # Convert data into image format\n",
    "    num_genes = tcga_scaled.shape[1]\n",
    "    image_size = int(np.ceil(np.sqrt(num_genes)))\n",
    "    padding = image_size**2 - num_genes\n",
    "\n",
    "    tcga_images = np.array([\n",
    "        np.pad(sample, (0, padding), mode='constant').reshape(image_size, image_size)\n",
    "        for sample in tcga_scaled\n",
    "    ])\n",
    "    met500_images = np.array([\n",
    "        np.pad(sample, (0, padding), mode='constant').reshape(image_size, image_size)\n",
    "        for sample in met500_scaled\n",
    "    ])\n",
    "\n",
    "    tcga_images = tcga_images[..., np.newaxis]\n",
    "    met500_images = met500_images[..., np.newaxis]\n",
    "\n",
    "    # Generate dummy labels for training\n",
    "    labels_tcga = np.random.randint(0, 2, tcga_images.shape[0])  # Replace with actual labels\n",
    "    labels_met500 = np.random.randint(0, 2, met500_images.shape[0])  # Replace with actual labels\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(tcga_images, labels_tcga, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"Loading preprocessed data for evaluation...\")\n",
    "    # Assume preprocessed data is stored as arrays or DataFrames\n",
    "    # These would match the results from preprocessing with RETRAIN_MODEL=True\n",
    "    # Placeholder examples:\n",
    "    image_size = 224  # Adjust this value based on your image size\n",
    "    met500_images = np.random.rand(100, image_size, image_size, 1)  # Placeholder for test data\n",
    "    labels_met500 = np.random.randint(0, 2, 100)  # Placeholder for test labels\n",
    "\n",
    "# Train or load the model\n",
    "if RETRAIN_MODEL:\n",
    "    # Build the model\n",
    "    input_layer = Input(shape=(image_size, image_size, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', strides=(5, 5))(input_layer)\n",
    "    flatten = Flatten()(conv1)\n",
    "    dropout = Dropout(0.5)(flatten)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(model_file)\n",
    "    print(f\"Model saved to: {model_file}\")\n",
    "else:\n",
    "    # Load the existing model\n",
    "    model = load_model(model_file)\n",
    "    print(f\"Model loaded from: {model_file}\")\n",
    "\n",
    "# Evaluate on MET500\n",
    "y_pred = (model.predict(met500_images) > 0.5).astype(int)\n",
    "\n",
    "# Results report\n",
    "accuracy = accuracy_score(labels_met500, y_pred)\n",
    "print(f\"\\nAccuracy on MET500: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels_met500, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(labels_met500, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Run this cell just if you want to preprocess data from the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "# Importing necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# URLs of the datasets\n",
    "tcga_url = \"https://toil-xena-hub.s3.us-east-1.amazonaws.com/download/tcga_RSEM_gene_fpkm.gz\"\n",
    "met500_url = \"https://ucsc-public-main-xena-hub.s3.us-east-1.amazonaws.com/download/MET500%2FgeneExpression%2FM.mx.log2.txt.gz\"\n",
    "\n",
    "# File paths for the processed datasets\n",
    "tcga_file_path = \"data/tcga_gene_expression_log2_common_genes.csv\"\n",
    "met500_file_path = \"data/met500_gene_expression_common_genes.csv\"\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Function to download and load the compressed file\n",
    "def download_and_load_gzip(url):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with gzip.open(BytesIO(response.content), 'rt') as f:\n",
    "        df = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "    return df\n",
    "\n",
    "# Check if files already exist\n",
    "if os.path.exists(tcga_file_path) and os.path.exists(met500_file_path):\n",
    "    print(f\"Processed files already exist:\")\n",
    "    print(f\"- TCGA: {tcga_file_path}\")\n",
    "    print(f\"- MET500: {met500_file_path}\")\n",
    "    print(\"\\nIf you want to preprocess the data again, delete the existing files and re-run this cell.\")\n",
    "else:\n",
    "    # Process the TCGA dataset\n",
    "    print(\"Downloading and processing TCGA data...\")\n",
    "    tcga_df = download_and_load_gzip(tcga_url)\n",
    "\n",
    "    # Process the MET500 dataset\n",
    "    print(\"Downloading and processing MET500 data...\")\n",
    "    met500_df = download_and_load_gzip(met500_url)\n",
    "\n",
    "    # Intersect common genes between TCGA and MET500\n",
    "    common_genes = tcga_df.index.intersection(met500_df.index)\n",
    "\n",
    "    # Filter both datasets for common genes\n",
    "    tcga_df_log2 = tcga_df.loc[common_genes]\n",
    "    met500_df_log2 = met500_df.loc[common_genes]\n",
    "\n",
    "    # Check dimensions after filtering\n",
    "    print(f\"Number of common genes: {len(common_genes)}\")\n",
    "    print(f\"Dimensions of TCGA dataset after filtering: {tcga_df_log2.shape}\")\n",
    "    print(f\"Dimensions of MET500 dataset after filtering: {met500_df_log2.shape}\")\n",
    "\n",
    "    # Save the processed datasets to local files\n",
    "    tcga_df_log2.to_csv(tcga_file_path)\n",
    "    met500_df_log2.to_csv(met500_file_path)\n",
    "\n",
    "    print(f\"Processed TCGA dataset saved to: {tcga_file_path}\")\n",
    "    print(f\"Processed MET500 dataset saved to: {met500_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
