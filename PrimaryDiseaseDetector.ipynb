{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrimaryDiseaseDetector.ipynb\n",
    "\n",
    "# Importación de librerías necesarias\n",
    "\n",
    "# Utilidades estándar\n",
    "import os\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuración\n",
    "RETRAIN_MODEL = False\n",
    "model_file = \"model/PrimaryDiseaseDetectorModel.keras\"\n",
    "\n",
    "# Función para descargar archivos desde Google Drive\n",
    "def download_from_google_drive(url, output_path):\n",
    "    file_id = url.split('/d/')[1].split('/')[0]\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n",
    "\n",
    "# URLs de Google Drive\n",
    "tcga_dataset_log2_url = \"https://drive.google.com/file/d/1-6OA1Q0TqFeooVHmURcZ_F9YjRh9D2cK/view?usp=drive_link\"\n",
    "met500_dataset_log2_url = \"https://drive.google.com/file/d/1nBzGFuq-ExWw0KC0dtagJqAOFjji8bQc/view?usp=drive_link\"\n",
    "phenotype_tcga_url = \"https://drive.google.com/file/d/1wNXgjZMQUDqNosG_q8qZNIIq0za-ghF0/view?usp=drive_link\"\n",
    "phenotype_met500_url = \"https://drive.google.com/file/d/1-7yVlLwIo2aD_eojIysUllnRXb3j-b7e/view?usp=drive_link\"\n",
    "\n",
    "# Directorios\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Cargar o descargar y procesar datos según RETRAIN_MODEL\n",
    "if RETRAIN_MODEL:\n",
    "    print(\"Descargando datos de TCGA...\")\n",
    "    download_from_google_drive(tcga_dataset_log2_url, \"data/tcga_gene_expression_log2_common_genes.csv\")\n",
    "\n",
    "    print(\"Descargando datos de MET500...\")\n",
    "    download_from_google_drive(met500_dataset_log2_url, \"data/met500_gene_expression_common_genes.csv\")\n",
    "\n",
    "    print(\"Descargando fenotipos de TCGA...\")\n",
    "    download_from_google_drive(phenotype_tcga_url, \"data/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\")\n",
    "\n",
    "    print(\"Descargando fenotipos de MET500...\")\n",
    "    download_from_google_drive(phenotype_met500_url, \"data/MET500_metadata.txt\")\n",
    "\n",
    "    # Cargar los datasets\n",
    "    tcga_df_log2 = pd.read_csv(\"data/tcga_gene_expression_log2_common_genes.csv\", index_col=0)\n",
    "    met500_df = pd.read_csv(\"data/met500_gene_expression_common_genes.csv\", index_col=0)\n",
    "    phenotype_tcga = pd.read_csv(\"data/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\", sep=\"\\t\").set_index(\"sample\")\n",
    "    phenotype_met500 = pd.read_csv(\"data/MET500_metadata.txt\", sep=\"\\t\").set_index(\"Sample_id\")\n",
    "\n",
    "    # Verificar las dimensiones de los datos\n",
    "    print(f\"Dimensiones de TCGA: {tcga_df_log2.shape}\")\n",
    "    print(f\"Dimensiones de MET500: {met500_df.shape}\")\n",
    "    print(f\"Dimensiones del fenotipo TCGA: {phenotype_tcga.shape}\")\n",
    "    print(f\"Dimensiones del fenotipo MET500: {phenotype_met500.shape}\")\n",
    "\n",
    "    # Normalización y preprocesamiento de datos\n",
    "    scaler = MinMaxScaler()\n",
    "    tcga_scaled = scaler.fit_transform(tcga_df_log2.T)\n",
    "    met500_scaled = scaler.transform(met500_df.T)\n",
    "\n",
    "    # Convertir a formato imagen\n",
    "    num_genes = tcga_scaled.shape[1]\n",
    "    image_size = int(np.ceil(np.sqrt(num_genes)))\n",
    "    padding = image_size**2 - num_genes\n",
    "\n",
    "    tcga_images = np.array([\n",
    "        np.pad(sample, (0, padding), mode='constant').reshape(image_size, image_size)\n",
    "        for sample in tcga_scaled\n",
    "    ])\n",
    "    met500_images = np.array([\n",
    "        np.pad(sample, (0, padding), mode='constant').reshape(image_size, image_size)\n",
    "        for sample in met500_scaled\n",
    "    ])\n",
    "\n",
    "    tcga_images = tcga_images[..., np.newaxis]\n",
    "    met500_images = met500_images[..., np.newaxis]\n",
    "\n",
    "    # Etiquetas ficticias para entrenamiento\n",
    "    labels_tcga = np.random.randint(0, 2, tcga_images.shape[0])  # Cambiar por etiquetas reales\n",
    "    labels_met500 = np.random.randint(0, 2, met500_images.shape[0])  # Cambiar por etiquetas reales\n",
    "\n",
    "    # Dividir en entrenamiento y validación\n",
    "    X_train, X_val, y_train, y_val = train_test_split(tcga_images, labels_tcga, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"Cargando datos preprocesados para evaluación...\")\n",
    "    # Supongamos que ya se tienen datos preprocesados guardados en arrays o DataFrames\n",
    "    # Estos serían los mismos resultados de haber preprocesado con RETRAIN_MODEL=True\n",
    "    # Placeholder de ejemplos:\n",
    "    image_size = 224  # Cambia este valor según el tamaño de tus imágenes\n",
    "    met500_images = np.random.rand(100, image_size, image_size, 1)  # Placeholder de datos de prueba\n",
    "    labels_met500 = np.random.randint(0, 2, 100)  # Placeholder de etiquetas de prueba\n",
    "\n",
    "# Entrenar el modelo o cargarlo\n",
    "if RETRAIN_MODEL:\n",
    "    # Construcción del modelo\n",
    "    input_layer = Input(shape=(image_size, image_size, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', strides=(5, 5))(input_layer)\n",
    "    flatten = Flatten()(conv1)\n",
    "    dropout = Dropout(0.5)(flatten)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8)\n",
    "\n",
    "    # Entrenamiento\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    model.save(model_file)\n",
    "    print(f\"Modelo guardado en: {model_file}\")\n",
    "else:\n",
    "    # Cargar el modelo existente\n",
    "    model = load_model(model_file)\n",
    "    print(f\"Modelo cargado desde: {model_file}\")\n",
    "\n",
    "# Evaluación en MET500\n",
    "y_pred = (model.predict(met500_images) > 0.5).astype(int)\n",
    "\n",
    "# Reporte de resultados\n",
    "accuracy = accuracy_score(labels_met500, y_pred)\n",
    "print(f\"\\nAccuracy on MET500: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels_met500, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "conf_matrix = confusion_matrix(labels_met500, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
